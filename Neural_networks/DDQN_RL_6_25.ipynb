{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "os.chdir(r\"C:\\Users\\cgree\\Documents\\Astra\\Space_weather5_22\\weakley_all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import base64\n",
    "import imageio\n",
    "import IPython\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "import pyvirtualdisplay\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.networks import q_network\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.utils import common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from subprocess import check_output\n",
    "#print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n",
    "import time\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import chainer\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "from plotly import tools\n",
    "from plotly.graph_objs import *\n",
    "from plotly.offline import init_notebook_mode, iplot, iplot_mpl\n",
    "init_notebook_mode()\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('combined_csv.csv')\n",
    "df.index = pd.to_datetime(df[['year','month','day' ,'hour','minute','second']])\n",
    "df = df.drop(columns=[\"year\", \"month\", \"day\", \"hour\", \"minute\", \"second\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.read_csv(r'C:\\Users\\cgree\\Documents\\Astra\\Space_weather5_22\\2018_sm_reg.csv')\n",
    "df3 = df3.set_index('Date_UTC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfx = df.join(df3, how='inner')#  how='left')   #'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfx= dfx.replace([99999.9, 9999.99, 999.99, 999999.0 ,999999.00,-99999.990000, '#REF!'], np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dfx['AE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_derivative(df, new_column_name, column_to_derive, shift_in_date = 1):\n",
    "    df[new_column_name] = (df[column_to_derive])-(df[column_to_derive].shift(1))/(shift_in_date)\n",
    "    return df[new_column_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_column_name = 'FOD' #first order derivative\n",
    "column_to_derive = 'gic'\n",
    "calculate_derivative(dfx, new_column_name, column_to_derive, shift_in_date = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#second order derivative\n",
    "new_column_name = 'SOD' #second order derivative\n",
    "column_to_derive = 'FOD'\n",
    "calculate_derivative(dfx, new_column_name, column_to_derive, shift_in_date = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dfx[['gic', 'FOD', 'SOD']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = new_df[2:] #dropping first two rows with Nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment1:\n",
    "    \n",
    "    def __init__(self, data, history_t=90):\n",
    "        self.data = data\n",
    "        self.history_t = history_t\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.t = 0\n",
    "        self.done = False\n",
    "        self.profits = 0\n",
    "        self.positions = []\n",
    "        self.position_value = 0\n",
    "        self.history = [0 for _ in range(self.history_t)]\n",
    "        return [self.position_value] + self.history # obs\n",
    "    \n",
    "    def step(self, act):\n",
    "        reward = 0\n",
    "        \n",
    "        # act = 0: stay, 1: up(left), 2: down(right)\n",
    "        if act == 1: #up\n",
    "            self.positions.append(self.data.iloc[self.t, :]['gic'])\n",
    "        elif act == 2: # down\n",
    "            if len(self.positions) == 0:\n",
    "                reward = - 1\n",
    "            else:\n",
    "                profits = 0\n",
    "                for p in self.positions:\n",
    "                    profits += 1 #(self.data.iloc[self.t, :]['gic'] - p)\n",
    "                reward += profits\n",
    "                self.profits += profits\n",
    "                self.positions = []\n",
    "        \n",
    "        # set next time\n",
    "        self.t += 1\n",
    "        self.position_value = 0\n",
    "        for p in self.positions:\n",
    "            self.position_value += (self.data.iloc[self.t, :]['gic'] - p)\n",
    "        self.history.pop(0)\n",
    "        self.history.append(self.data.iloc[self.t, :]['FOD'])\n",
    "        \n",
    "        # clipping reward\n",
    "        if reward > 0:\n",
    "            reward = 1\n",
    "        elif reward < 0:\n",
    "            reward = -1\n",
    "        \n",
    "        return [self.position_value] + self.history, reward, self.done # obs, reward, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_split = '2018-03-15 00:00:00'\n",
    "# train = data[:date_split]\n",
    "# test = data[date_split:]\n",
    "train = data['2018-03-01 00:00:00':'2018-03-15 00:00:00']\n",
    "test = data['2018-03-15 01:00:00':'2018-03-31 23:59:00']\n",
    "len(train), len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Environment1(train)\n",
    "print(env.reset())\n",
    "for _ in range(3):\n",
    "    pact = np.random.randint(3)\n",
    "    print(env.step(pact))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Double DQN\n",
    "\n",
    "def train_ddqn(env):\n",
    "\n",
    "    class Q_Network(chainer.Chain): #Chainer is similar to Open AI its for RL archetecture\n",
    "    #'''look over documentaion if you forget what this means--https://chainer.org/general/2017/02/22/ChainerRL-Deep-Reinforcement-Learning-Library.html'''\n",
    "        def __init__(self, input_size, hidden_size, output_size):\n",
    "            super(Q_Network, self).__init__(\n",
    "                fc1 = L.Linear(input_size, hidden_size),\n",
    "                fc2 = L.Linear(hidden_size, hidden_size),\n",
    "                fc3 = L.Linear(hidden_size, output_size)\n",
    "            )\n",
    "\n",
    "        def __call__(self, x):\n",
    "            h = F.relu(self.fc1(x))\n",
    "            h = F.relu(self.fc2(h))\n",
    "            y = self.fc3(h)\n",
    "            return y\n",
    "\n",
    "        def reset(self):\n",
    "            self.zerograds()\n",
    "\n",
    "    #Then “Agent” can be defined given the model, an optimizer in Chainer, and algorithm-specific \n",
    "    #parameters. Agents execute the training of the model through interactions with the environment.\n",
    "    Q = Q_Network(input_size=env.history_t+1, hidden_size=100, output_size=3)\n",
    "    Q_ast = copy.deepcopy(Q)\n",
    "    optimizer = chainer.optimizers.Adam()\n",
    "    optimizer.setup(Q)\n",
    "\n",
    "    epoch_num = 50\n",
    "    step_max = len(env.data)-1\n",
    "    memory_size = 200\n",
    "    batch_size = 50\n",
    "    epsilon = 1.0\n",
    "    epsilon_decrease = 1e-3 #these are the parameters for the Q funtion.\n",
    "    epsilon_min = 0.1\n",
    "    start_reduce_epsilon = 200\n",
    "    train_freq = 10\n",
    "    update_q_freq = 20\n",
    "    gamma = 0.97  # Set the discount factor that discounts future rewards.\n",
    "    show_log_freq = 5\n",
    "\n",
    "    memory = []\n",
    "    total_step = 0\n",
    "    total_rewards = []\n",
    "    total_losses = []\n",
    "\n",
    "    start = time.time()\n",
    "    for epoch in range(epoch_num):\n",
    "\n",
    "        pobs = env.reset()\n",
    "        step = 0\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        total_loss = 0\n",
    "\n",
    "        while not done and step < step_max:\n",
    "\n",
    "            # select act\n",
    "            pact = np.random.randint(3)\n",
    "            if np.random.rand() > epsilon:\n",
    "                pact = Q(np.array(pobs, dtype=np.float32).reshape(1, -1))\n",
    "                pact = np.argmax(pact.data)\n",
    "\n",
    "            # act\n",
    "            obs, reward, done = env.step(pact)\n",
    "\n",
    "            # add memory\n",
    "            memory.append((pobs, pact, reward, obs, done))\n",
    "            if len(memory) > memory_size:\n",
    "                memory.pop(0)\n",
    "\n",
    "            # train or update q\n",
    "            if len(memory) == memory_size:\n",
    "                if total_step % train_freq == 0:\n",
    "                    shuffled_memory = np.random.permutation(memory)\n",
    "                    memory_idx = range(len(shuffled_memory))\n",
    "                    for i in memory_idx[::batch_size]:\n",
    "                        batch = np.array(shuffled_memory[i:i+batch_size])\n",
    "                        b_pobs = np.array(batch[:, 0].tolist(), dtype=np.float32).reshape(batch_size, -1)\n",
    "                        b_pact = np.array(batch[:, 1].tolist(), dtype=np.int32)\n",
    "                        b_reward = np.array(batch[:, 2].tolist(), dtype=np.int32)\n",
    "                        b_obs = np.array(batch[:, 3].tolist(), dtype=np.float32).reshape(batch_size, -1)\n",
    "                        b_done = np.array(batch[:, 4].tolist(), dtype=np.bool)\n",
    "\n",
    "                        q = Q(b_pobs)\n",
    "                        \"\"\" <<< DQN -> Double DQN\n",
    "                        maxq = np.max(Q_ast(b_obs).data, axis=1)\n",
    "                        === \"\"\"\n",
    "                        indices = np.argmax(q.data, axis=1)\n",
    "                        maxqs = Q_ast(b_obs).data\n",
    "                        \"\"\" >>> \"\"\"\n",
    "                        target = copy.deepcopy(q.data)\n",
    "                        for j in range(batch_size):\n",
    "                            \"\"\" <<< DQN -> Double DQN\n",
    "                            target[j, b_pact[j]] = b_reward[j]+gamma*maxq[j]*(not b_done[j])\n",
    "                            === \"\"\"\n",
    "                            target[j, b_pact[j]] = b_reward[j]+gamma*maxqs[j, indices[j]]*(not b_done[j])\n",
    "                            \"\"\" >>> \"\"\"\n",
    "                        Q.reset()\n",
    "                        loss = F.mean_squared_error(q, target)\n",
    "                        total_loss += loss.data\n",
    "                        loss.backward()\n",
    "                        optimizer.update()\n",
    "\n",
    "                if total_step % update_q_freq == 0:\n",
    "                    Q_ast = copy.deepcopy(Q)\n",
    "\n",
    "            # epsilon\n",
    "            if epsilon > epsilon_min and total_step > start_reduce_epsilon:\n",
    "                epsilon -= epsilon_decrease\n",
    "\n",
    "            # next step\n",
    "            total_reward += reward\n",
    "            pobs = obs\n",
    "            step += 1\n",
    "            total_step += 1\n",
    "\n",
    "        total_rewards.append(total_reward)\n",
    "        total_losses.append(total_loss)\n",
    "\n",
    "        if (epoch+1) % show_log_freq == 0:\n",
    "            log_reward = sum(total_rewards[((epoch+1)-show_log_freq):])/show_log_freq\n",
    "            log_loss = sum(total_losses[((epoch+1)-show_log_freq):])/show_log_freq\n",
    "            elapsed_time = time.time()-start\n",
    "            print('\\t'.join(map(str, [epoch+1, epsilon, total_step, log_reward, log_loss, elapsed_time])))\n",
    "            start = time.time()\n",
    "            \n",
    "    return Q, total_losses, total_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q, total_losses, total_rewards = train_ddqn(Environment1(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_train_test_by_q(train_env, test_env, Q, algorithm_name):\n",
    "    \n",
    "    # train\n",
    "    pobs = train_env.reset()\n",
    "    train_acts = []\n",
    "    train_rewards = []\n",
    "\n",
    "    for _ in range(len(train_env.data)-1):\n",
    "        \n",
    "        pact = Q(np.array(pobs, dtype=np.float32).reshape(1, -1))\n",
    "        pact = np.argmax(pact.data)\n",
    "        train_acts.append(pact)\n",
    "            \n",
    "        obs, reward, done = train_env.step(pact)\n",
    "        train_rewards.append(reward)\n",
    "\n",
    "        pobs = obs\n",
    "        \n",
    "    train_profits = train_env.profits\n",
    "    \n",
    "    # test\n",
    "    pobs = test_env.reset()\n",
    "    test_acts = []\n",
    "    test_rewards = []\n",
    "\n",
    "    for _ in range(len(test_env.data)-1):\n",
    "    \n",
    "        pact = Q(np.array(pobs, dtype=np.float32).reshape(1, -1))\n",
    "        pact = np.argmax(pact.data)\n",
    "        test_acts.append(pact)\n",
    "            \n",
    "        obs, reward, done = test_env.step(pact)\n",
    "        test_rewards.append(reward)\n",
    "\n",
    "        pobs = obs\n",
    "        \n",
    "    test_profits = test_env.profits\n",
    "    \n",
    "    # plot\n",
    "    train_copy = train_env.data.copy()\n",
    "    test_copy = test_env.data.copy()\n",
    "    train_copy['act'] = train_acts + [np.nan]\n",
    "    train_copy['reward'] = train_rewards + [np.nan]\n",
    "    test_copy['act'] = test_acts + [np.nan]\n",
    "    test_copy['reward'] = test_rewards + [np.nan]\n",
    "    train0 = train_copy[train_copy['act'] == 0]\n",
    "    train1 = train_copy[train_copy['act'] == 1]\n",
    "    train2 = train_copy[train_copy['act'] == 2]\n",
    "    test0 = test_copy[test_copy['act'] == 0]\n",
    "    test1 = test_copy[test_copy['act'] == 1]\n",
    "    test2 = test_copy[test_copy['act'] == 2]\n",
    "    act_color0, act_color1, act_color2 = 'gray', 'cyan', 'magenta'\n",
    "\n",
    "    data = [\n",
    "        Scatter(x=train0.index, y=train0['gic'],mode='markers',marker_color=act_color0,text=train0['gic']), # hover text goes here,\n",
    "        Scatter(x=train1.index, y=train0['gic'],mode='markers',marker_color=act_color1,text=train1['gic']),\n",
    "        Scatter(x=train2.index, y=train0['gic'],mode='markers',marker_color=act_color2,text=train2['gic']),\n",
    "        Scatter(x=test0.index, y=test0['gic'],mode='markers',marker_color=act_color0,text=train0['gic']),\n",
    "        Scatter(x=test1.index, y=test1['gic'],mode='markers',marker_color=act_color1,text=train1['gic']),\n",
    "        Scatter(x=test2.index, y=test2['gic'],mode='markers',marker_color=act_color2,text=train2['gic'])\n",
    "    ]\n",
    "#     data = [\n",
    "#         Candlestick(x=train0.index, open=train0['gic'], high=train0['FOD'], low=train0['SOD'], close=train0['gic'], increasing=dict(line=dict(color=act_color0)), decreasing=dict(line=dict(color=act_color0))),\n",
    "#         Candlestick(x=train1.index, open=train1['gic'], high=train1['FOD'], low=train1['SOD'], close=train1['gic'], increasing=dict(line=dict(color=act_color1)), decreasing=dict(line=dict(color=act_color1))),\n",
    "#         Candlestick(x=train2.index, open=train2['gic'], high=train2['FOD'], low=train2['SOD'], close=train2['gic'], increasing=dict(line=dict(color=act_color2)), decreasing=dict(line=dict(color=act_color2))),\n",
    "#         Candlestick(x=test0.index, open=test0['gic'], high=test0['FOD'], low=test0['SOD'], close=test0['gic'], increasing=dict(line=dict(color=act_color0)), decreasing=dict(line=dict(color=act_color0))),\n",
    "#         Candlestick(x=test1.index, open=test1['gic'], high=test1['FOD'], low=test1['SOD'], close=test1['gic'], increasing=dict(line=dict(color=act_color1)), decreasing=dict(line=dict(color=act_color1))),\n",
    "#         Candlestick(x=test2.index, open=test2['gic'], high=test2['FOD'], low=test2['SOD'], close=test2['gic'], increasing=dict(line=dict(color=act_color2)), decreasing=dict(line=dict(color=act_color2)))\n",
    "#     ]\n",
    "    title = '{}: train s-reward {}, profits {}, test s-reward {}, profits {}'.format(\n",
    "        algorithm_name,\n",
    "        int(sum(train_rewards)),\n",
    "        int(train_profits),\n",
    "        int(sum(test_rewards)),\n",
    "        int(test_profits)\n",
    "    )\n",
    "    layout = {\n",
    "        'title': title,\n",
    "        'showlegend': False,\n",
    "         'shapes': [\n",
    "             {'x0': date_split, 'x1': date_split, 'y0': 0, 'y1': 1, 'xref': 'x', 'yref': 'paper', 'line': {'color': 'rgb(0,0,0)', 'width': 1}}\n",
    "         ],\n",
    "        'annotations': [\n",
    "            {'x': date_split, 'y': 1.0, 'xref': 'x', 'yref': 'paper', 'showarrow': False, 'xanchor': 'left', 'text': ' test data'},\n",
    "            {'x': date_split, 'y': 1.0, 'xref': 'x', 'yref': 'paper', 'showarrow': False, 'xanchor': 'right', 'text': 'train data '}\n",
    "        ]\n",
    "    }\n",
    "    figure = Figure(data=data, layout=layout)\n",
    "    iplot(figure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot_train_test_by_q(Environment1(train), Environment1(test), Q, 'DQN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
